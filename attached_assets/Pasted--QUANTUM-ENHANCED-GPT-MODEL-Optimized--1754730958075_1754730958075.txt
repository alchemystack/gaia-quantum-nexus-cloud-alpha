
# ============================================
# QUANTUM-ENHANCED GPT MODEL (Optimized)
# ============================================
@app.cls(
    image=image,
    gpu=gpu_config,
    volumes={"/cache": model_volume},
    timeout=3600,
    min_containers=1,  # KEEPS CONTAINER WARM - NO COLD STARTS (updated from keep_warm)
    memory=131072,
    cpu=16.0,
    concurrency_limit=5,  # Updated from allow_concurrent_inputs
    secrets=[
        modal.Secret.from_name("qrng-api-key"),
    ]
)
class QuantumGPT120BTransformers:
    """
    OpenAI GPT-OSS 120B with Direct Logit Modification via QRNG
    OPTIMIZED: Model weights persist in volume across kernel resets
    """

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.qrng = None
        self.device = "cuda"
        self.cache_dir = "/cache/models"
        self.model_loaded = False

    @modal.enter()
    def load_model(self):
        """
        Initialize model from CACHED weights and QRNG service
        This is FAST because weights are already on disk
        """
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import os
        from pathlib import Path

        print("ðŸš€ QUANTUM GPT-OSS 120B INITIALIZATION (OPTIMIZED)")
        print("=" * 60)

        # Initialize QRNG
        qrng_key = os.environ.get("QRNG_API_KEY")
        if not qrng_key:
            raise Exception(
                "QRNG_API_KEY not found - HALTING (no fallback allowed)")

        self.qrng = QRNGService(qrng_key)
        print("âœ… QRNG service initialized")

        # Check if model is cached
        cache_path = Path(self.cache_dir)
        model_id = "openai/gpt-oss-120b"

        if not (cache_path / "gpt-oss-120b").exists():
            print("âš ï¸  Model not found in cache, downloading...")
            # This should rarely happen if download_model_if_needed was called
            download_result = download_model_if_needed.remote()
            print(f"Download result: {download_result}")

        print(f"\nðŸ“¥ Loading model from CACHED weights...")
        print(f"   Cache location: {self.cache_dir}")
        print("   Using 8-bit quantization for 80GB VRAM")

        start_time = time.time()

        # Load from CACHED weights (FAST!)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=self.cache_dir,
            load_in_8bit=True,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            local_files_only=True,  # Use ONLY cached files
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            cache_dir=self.cache_dir,
            trust_remote_code=True,
            local_files_only=True,  # Use ONLY cached files
        )

        # Set padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        load_time = time.time() - start_time

        print(f"âœ… Model loaded in {load_time:.1f} seconds (from cache)")
        print(
            f"   Parameters: {sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B"
        )
        print(f"   Device: {next(self.model.parameters()).device}")
        print("âœ… Ready for quantum-enhanced inference")

        self.model_loaded = True

    @modal.exit()
    def cleanup(self):
        """Cleanup on container exit (rarely happens with keep_warm=1)"""
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        torch.cuda.empty_cache()

    def apply_quantum_modification(
            self,
            logits: "torch.Tensor",  # Using string type hint to avoid import issues
            quantum_profile: str = "medium",
            temperature: float = 1.0) -> Tuple["torch.Tensor", Dict[str, float]]:
        """
        Apply QRNG modification directly to logits
        Returns modified logits and diagnostics
        """
        import torch

        # Quantum intensity based on profile
        intensity_map = {
            "strict": 0.0,  # No quantum (control)
            "light": 0.1,  # Subtle quantum influence
            "medium": 0.3,  # Balanced quantum
            "spicy": 0.5,  # Strong quantum
            "chaos": 0.8  # Maximum quantum chaos
        }

        intensity = intensity_map.get(quantum_profile, 0.3)

        if intensity == 0.0:
            return logits, {"intensity": 0.0, "modification": 0.0}

        # Get quantum noise shaped like logits
        batch_size, vocab_size = logits.shape
        quantum_noise = self.qrng.get_quantum_noise(shape=(batch_size,
                                                           vocab_size),
                                                    intensity=intensity)

        # Convert to torch tensor
        quantum_noise = torch.from_numpy(quantum_noise).to(logits.device).to(
            logits.dtype)

        # Store original for diagnostics
        original_max = logits.max().item()

        # Apply quantum modification (additive noise)
        modified_logits = logits + quantum_noise

        # Calculate diagnostics
        modification = (modified_logits - logits).abs().mean().item()
        max_change = (modified_logits - logits).abs().max().item()

        # Apply temperature scaling after quantum modification
        modified_logits = modified_logits / temperature

        diagnostics = {
            "intensity": intensity,
            "modification": modification,
            "max_change": max_change,
            "original_max_logit": original_max,
            "entropy_consumed": batch_size * vocab_size * 4  # bytes
        }

        return modified_logits, diagnostics

    @modal.method()
    async def generate(self,
                       prompt: str,
                       max_tokens: int = 128,
                       temperature: float = 0.7,
                       quantum_profile: str = "medium",
                       diagnostics: bool = True) -> Dict[str, Any]:
        """
        Generate text with direct QRNG logit modification
        """
        import torch
        import torch.nn.functional as F

        if not self.model_loaded:
            return {"status": "error", "message": "Model not loaded"}

        try:
            # Tokenize input
            inputs = self.tokenizer(prompt,
                                    return_tensors="pt",
                                    padding=True,
                                    truncation=True,
                                    max_length=2048).to(self.device)

            input_ids = inputs.input_ids
            attention_mask = inputs.attention_mask

            # Generate with quantum modification
            generated_ids = []
            quantum_diagnostics = {"applications": [], "total_entropy": 0}

            with torch.no_grad():
                for step in range(max_tokens):
                    # Forward pass to get logits
                    outputs = self.model(input_ids=input_ids,
                                         attention_mask=attention_mask,
                                         use_cache=True)

                    # Get logits for next token
                    next_token_logits = outputs.logits[:, -1, :]

                    # APPLY QUANTUM MODIFICATION TO LOGITS
                    modified_logits, step_diagnostics = self.apply_quantum_modification(
                        next_token_logits,
                        quantum_profile=quantum_profile,
                        temperature=temperature)

                    # Sample from modified distribution
                    probs = F.softmax(modified_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)

                    # Append to generated sequence
                    generated_ids.append(next_token.item())
                    input_ids = torch.cat([input_ids, next_token], dim=-1)
                    attention_mask = torch.cat([
                        attention_mask,
                        torch.ones((1, 1), device=self.device)
                    ],
                                               dim=-1)

                    # Store diagnostics
                    if diagnostics:
                        quantum_diagnostics["applications"].append({
                            "step":
                            step,
                            "logit_diff":
                            step_diagnostics["modification"],
                            "max_change":
                            step_diagnostics["max_change"]
                        })
                        quantum_diagnostics[
                            "total_entropy"] += step_diagnostics[
                                "entropy_consumed"]

                    # Stop on EOS token
                    if next_token.item() == self.tokenizer.eos_token_id:
                        break

            # Decode generated text
            generated_text = self.tokenizer.decode(generated_ids,
                                                   skip_special_tokens=True)

            # Calculate average modifications
            if diagnostics and quantum_diagnostics["applications"]:
                apps = quantum_diagnostics["applications"]
                quantum_diagnostics["avg_logit_modification"] = sum(
                    a["logit_diff"] for a in apps) / len(apps)
                quantum_diagnostics["max_modification"] = max(a["max_change"]
                                                              for a in apps)

            return {
                "status": "success",
                "generated_text": generated_text,
                "quantum_profile": quantum_profile,
                "tokens_generated": len(generated_ids),
                "quantum_diagnostics":
                quantum_diagnostics if diagnostics else None
            }

        except Exception as e:
            return {"status": "error", "message": str(e)}

    @modal.method()
    async def health(self) -> Dict[str, Any]:
        """Health check endpoint"""
        return {
            "status": "healthy",
            "model": "OpenAI GPT-OSS 120B",
            "framework": "Transformers (Optimized)",
            "quantum": "ready",
            "model_loaded": self.model_loaded,
            "cache_dir": self.cache_dir,
            "capabilities": {
                "direct_logit_modification": True,
                "quantum_profiles":
                ["strict", "light", "medium", "spicy", "chaos"],
                "no_pseudorandom_fallback": True,
                "persistent_cache": True
            }
        }

