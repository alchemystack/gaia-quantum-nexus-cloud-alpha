
# ============================================
# CELL 2: CORE QUANTUM TRANSFORMERS MODEL
# ============================================
"""
GPT-OSS 120B with Transformers for DIRECT LOGIT MODIFICATION
This class handles the actual model loading and quantum-modified generation

AUTHENTICATION UPDATE (January 2025):
- Modal now requires 3 secrets: MODAL_API_KEY, MODAL_TOKEN_SECRET, MODAL_ENDPOINT  
- Authentication uses Basic Auth with token-id:token-secret format
- See MODAL_AUTHENTICATION_SETUP.md for complete guide
"""

import modal

# CRITICAL: Define Image BEFORE using it in @app.cls decorator
# Install transformers and dependencies for GPT-OSS 120B
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "torch",
        "transformers>=4.36.0",
        "accelerate",
        "bitsandbytes",  # For 8-bit quantization
        "sentencepiece",
        "protobuf",
        "numpy",
        "requests"
    )
)

# Define the persistent volume for model storage (40GB+ for 120B model)
model_volume = modal.Volume.from_name(
    "gpt-oss-120b-volume",
    create_if_missing=True
)

@app.cls(
    image=image,
    gpu="A100-80GB",  # Required for 120B model
    memory=131072,  # 128GB RAM
    cpu=16,  # Enhanced CPU cores
    timeout=900,
    volumes={"/cache": model_volume},
    secrets=[modal.Secret.from_name("qrng-api-key")],
    concurrency_limit=1,
    max_containers=1  # Changed from container_idle_timeout
)
class QuantumGPT120BTransformers:
    """
    Production-ready GPT-OSS 120B with direct quantum logit modification.
    Uses Transformers library for DIRECT access to raw logits BEFORE sampling.
    """
    
    def load_model(self):
        """Load the 120B model with 8-bit quantization"""
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import os
        
        print(f"[{self.get_timestamp()}] Initializing GPT-OSS 120B with Transformers...")
        
        # Model configuration
        model_id = "openai/gpt-oss-120b"
        cache_dir = "/cache/models"
        
        # Load tokenizer
        print(f"[{self.get_timestamp()}] Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # Check for CUDA availability
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA not available! A100 GPU required for 120B model.")
        
        print(f"[{self.get_timestamp()}] GPU detected: {torch.cuda.get_device_name(0)}")
        print(f"[{self.get_timestamp()}] VRAM available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # Load model with 8-bit quantization to fit in 80GB VRAM
        print(f"[{self.get_timestamp()}] Loading 120B model with 8-bit quantization...")
        print(f"[{self.get_timestamp()}] This may take 2-3 minutes on first load...")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=cache_dir,
            device_map="auto",
            load_in_8bit=True,  # Critical for fitting in 80GB
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        print(f"[{self.get_timestamp()}] ✅ Model loaded successfully!")
        print(f"[{self.get_timestamp()}] Model size: 117B total parameters, 5.1B active")
        
        # Initialize QRNG client
        self.qrng_api_key = os.environ.get("QRNG_API_KEY")
        print(f"[{self.get_timestamp()}] QRNG API key configured: {'✓' if self.qrng_api_key else '✗'}")
        
        self.model_loaded = True
    
    def get_timestamp(self):
        """Get current timestamp for logging"""
        import datetime
        return datetime.datetime.now().strftime('%H:%M:%S')
    
    def get_quantum_entropy(self, num_values):
        """Fetch quantum random numbers from QRNG API"""
        import requests
        import numpy as np
        
        if not self.qrng_api_key:
            # Return zeros if no QRNG (control mode)
            return np.zeros(num_values)
        
        try:
            response = requests.get(
                "https://qrng.qblockchain.es/api/random",
                params={"size": num_values * 4},  # 4 bytes per float32
                headers={"Authorization": f"Bearer {self.qrng_api_key}"},
                timeout=5
            )
            
            if response.status_code == 200:
                data = response.json()
                if "randomData" in data:
                    # Convert hex to floats normalized to [-1, 1]
                    hex_data = data["randomData"]
                    bytes_data = bytes.fromhex(hex_data)
                    values = np.frombuffer(bytes_data, dtype=np.float32)[:num_values]
                    return 2 * (values - 0.5)  # Normalize to [-1, 1]
            
            print(f"[{self.get_timestamp()}] QRNG fetch failed, using zeros")
            return np.zeros(num_values)
            
        except Exception as e:
            print(f"[{self.get_timestamp()}] QRNG error: {e}")
            return np.zeros(num_values)
    
    @modal.method()
    def health(self):
        """Health check endpoint"""
        return {
            "status": "healthy",
            "model": "openai/gpt-oss-120b",
            "quantization": "8-bit",
            "device": "A100-80GB",
            "quantum_enabled": bool(self.qrng_api_key),
            "container_warm": hasattr(self, 'model_loaded') and self.model_loaded
        }
    
    @modal.method()
    def generate(
        self,
        prompt: str,
        max_tokens: int = 100,
        temperature: float = 0.7,
        quantum_profile: str = "medium",
        diagnostics: bool = False
    ):
        """
        Generate text with DIRECT QUANTUM LOGIT MODIFICATION
        
        Quantum profiles control the intensity of QRNG modification:
        - strict: No modification (control)
        - light: 10% quantum influence
        - medium: 30% quantum influence (balanced)
        - spicy: 50% quantum influence
        - chaos: 80% quantum influence (maximum creativity)
        """
        import torch
        import numpy as np
        
        # Load model if not already loaded
        if not hasattr(self, 'model_loaded') or not self.model_loaded:
            self.load_model()
        
        print(f"[{self.get_timestamp()}] Generating with quantum profile: {quantum_profile}")
        
        # Tokenize input
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        
        # Get quantum influence level
        quantum_strengths = {
            "strict": 0.0,
            "light": 0.1,
            "medium": 0.3,
            "spicy": 0.5,
            "chaos": 0.8
        }
        quantum_strength = quantum_strengths.get(quantum_profile, 0.3)
        
        # Generate tokens with quantum modification
        generated_tokens = []
        quantum_applications = []
        
        with torch.no_grad():
            for i in range(max_tokens):
                # Forward pass to get logits
                outputs = self.model(**inputs)
                logits = outputs.logits[0, -1, :]  # Shape: [vocab_size]
                
                # CRITICAL: Apply quantum modification DIRECTLY to logits
                if quantum_strength > 0:
                    vocab_size = logits.shape[0]
                    quantum_noise = self.get_quantum_entropy(min(vocab_size, 50000))
                    
                    # Pad or truncate to match vocab size
                    if len(quantum_noise) < vocab_size:
                        quantum_noise = np.pad(quantum_noise, (0, vocab_size - len(quantum_noise)))
                    else:
                        quantum_noise = quantum_noise[:vocab_size]
                    
                    # Convert to tensor and apply modification
                    quantum_tensor = torch.tensor(quantum_noise, device="cuda", dtype=logits.dtype)
                    logit_diff = quantum_strength * quantum_tensor * logits.std()
                    
                    # Apply quantum modification
                    logits = logits + logit_diff
                    
                    # Track quantum application
                    quantum_applications.append({
                        "token_index": i,
                        "logit_diff": float(logit_diff.abs().mean()),
                        "max_change": float(logit_diff.abs().max())
                    })
                
                # Apply temperature and sample
                logits = logits / temperature
                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # Decode and append
                token_text = self.tokenizer.decode(next_token[0])
                generated_tokens.append(token_text)
                
                # Update inputs for next iteration
                inputs = torch.cat([inputs.input_ids, next_token.unsqueeze(0)], dim=-1)
                
                # Stop if EOS token
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        # Combine generated text
        generated_text = "".join(generated_tokens)
        
        # Prepare response
        response = {
            "status": "success",
            "generated_text": generated_text,
            "tokens_generated": len(generated_tokens),
            "quantum_profile": quantum_profile,
            "quantum_strength": quantum_strength
        }
        
        # Add diagnostics if requested
        if diagnostics and quantum_applications:
            avg_modification = np.mean([q["logit_diff"] for q in quantum_applications])
            max_modification = np.max([q["max_change"] for q in quantum_applications])
            
            response["quantum_diagnostics"] = {
                "applications": quantum_applications[:10],  # First 10 for brevity
                "avg_logit_modification": float(avg_modification),
                "max_logit_modification": float(max_modification),
                "modified_token_count": len(quantum_applications),
                "entropy_consumed": len(quantum_applications) * 50000 * 4  # Approximate bytes
            }
        
        print(f"[{self.get_timestamp()}] Generated {len(generated_tokens)} tokens")
        return response

print("✅ Cell 2: QuantumGPT120BTransformers class defined")
print("Features:")
print("  • 120B parameters (5.1B active)")
print("  • 8-bit quantization for 80GB VRAM")
print("  • Direct logit modification with QRNG")
print("  • 5 quantum profiles (strict → chaos)")

