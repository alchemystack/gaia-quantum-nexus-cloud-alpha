"""
MODAL NOTEBOOK - GAIA QUANTUM NEXUS WITH OPENAI GPT-OSS 120B
==============================================================
Copy this entire code into Modal's web playground at:
https://modal.com/playground

Model: OpenAI GPT-OSS 120B (117B parameters, 5.1B active)
- Apache 2.0 licensed open-weight model
- Native MXFP4 quantization for single 80GB GPU deployment
- Harmony response format required
- Configurable reasoning levels (low/medium/high)

Enhanced configuration:
- 1x A100 80GB VRAM GPU (supports native MXFP4)
- 16 CPU cores
- 128GB system RAM
- Split cells for efficient model management

This notebook is split into cells:
Cell 1: Model upload from local storage (handles split GGUF files)
Cell 2: Server initialization and endpoints
"""

# ============================================
# CELL 1: MODEL UPLOAD AND PREPARATION
# ============================================
# Run this cell first to upload your GGUF model

import modal
import os
import time
import subprocess
from typing import Dict, Any, Optional
import json
import asyncio
import aiohttp

# Create Modal app
app = modal.App("gaia-quantum-120b-enhanced")

# Enhanced GPU configuration - A100 80GB with maximum resources
gpu_config = modal.gpu.A100(count=1, memory=80)  # 80GB VRAM

# Model storage - persistent volume for the 120B model
volume = modal.Volume.from_name("gaia-quantum-models-enhanced", create_if_missing=True)

# Container image with llama.cpp server
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install(
        "wget", "git", "build-essential", "cmake", "curl",
        "libssl-dev", "pkg-config", "libcurl4-openssl-dev"
    )
    .run_commands(
        # Install llama.cpp with server support
        "git clone https://github.com/ggerganov/llama.cpp /llama.cpp",
        "cd /llama.cpp && make -j16 LLAMA_CUDA=1 llama-server",  # Use 16 cores
        "ln -s /llama.cpp/llama-server /usr/local/bin/llama-server",
    )
    .pip_install(
        "huggingface-hub",
        "aiohttp",
        "numpy",
        "fastapi",
        "requests",
    )
)

@app.function(
    image=image,
    volumes={"/models": volume},
    timeout=7200,  # 2 hours for model upload
    cpu=16.0,      # 16 CPU cores
    memory=131072, # 128GB RAM
)
def upload_model_from_local(
    model_path_part1: Optional[str] = "D:.cashe\lm-studio\models\lmstudio-community\gpt-oss-120b-GGUF\gpt-oss-120b-MXFP4-00001-of-00002.gguf",
    model_path_part2: Optional[str] = "D:.cashe\lm-studio\models\lmstudio-community\gpt-oss-120b-GGUF\gpt-oss-120b-MXFP4-00002-of-00002.gguf"
):
    """
    Cell 1: Upload OpenAI GPT-OSS 120B GGUF model (split MXFP4 format)
    
    This handles the split GGUF files (MXFP4 quantization):
    - Part 1: gpt-oss-120b-MXFP4-00001-of-00002.gguf
    - Part 2: gpt-oss-120b-MXFP4-00002-of-00002.gguf
    
    Default paths point to LM Studio cache location.
    """
    import shutil
    from huggingface_hub import hf_hub_download
    
    print("üöÄ CELL 1: OPENAI GPT-OSS 120B MODEL PREPARATION")
    print("=" * 60)
    print("Model: OpenAI GPT-OSS 120B")
    print("Format: MXFP4 quantized GGUF (split into 2 parts)")
    print("License: Apache 2.0")
    print("=" * 60)
    
    model_dir = "/models/openai-gpt-oss-120b"
    os.makedirs(model_dir, exist_ok=True)
    
    # Check if both model parts already exist
    part1_path = f"{model_dir}/gpt-oss-120b-MXFP4-00001-of-00002.gguf"
    part2_path = f"{model_dir}/gpt-oss-120b-MXFP4-00002-of-00002.gguf"
    
    if os.path.exists(part1_path) and os.path.exists(part2_path):
        size1_gb = os.path.getsize(part1_path) / (1024**3)
        size2_gb = os.path.getsize(part2_path) / (1024**3)
        total_gb = size1_gb + size2_gb
        print(f"‚úÖ Model already exists:")
        print(f"   Part 1: {part1_path} ({size1_gb:.2f} GB)")
        print(f"   Part 2: {part2_path} ({size2_gb:.2f} GB)")
        print(f"   Total: {total_gb:.2f} GB")
        return {"status": "exists", "path": part1_path, "size_gb": total_gb}
    
    # Upload from local files
    if model_path_part1 and model_path_part2:
        if os.path.exists(model_path_part1) and os.path.exists(model_path_part2):
            print(f"üì§ Uploading OpenAI GPT-OSS 120B from local storage...")
            print(f"   Part 1: {model_path_part1}")
            print(f"   Part 2: {model_path_part2}")
            
            # Copy both parts
            shutil.copy2(model_path_part1, part1_path)
            shutil.copy2(model_path_part2, part2_path)
            print("‚úÖ Both model parts uploaded successfully!")
        else:
            print("‚ö†Ô∏è Local files not found, downloading from HuggingFace...")
            # Fallback to HuggingFace download
            try:
                print("üì• Downloading OpenAI GPT-OSS 120B from HuggingFace...")
                print("   Repository: openai/gpt-oss-120b")
                
                # Download part 1
                hf_hub_download(
                    repo_id="lmstudio-community/gpt-oss-120b-GGUF",
                    filename="gpt-oss-120b-MXFP4-00001-of-00002.gguf",
                    local_dir=model_dir,
                    local_dir_use_symlinks=False
                )
                
                # Download part 2
                hf_hub_download(
                    repo_id="lmstudio-community/gpt-oss-120b-GGUF",
                    filename="gpt-oss-120b-MXFP4-00002-of-00002.gguf",
                    local_dir=model_dir,
                    local_dir_use_symlinks=False
                )
                
                print("‚úÖ Model downloaded from HuggingFace")
            except Exception as e:
                print(f"‚ö†Ô∏è Download error: {e}")
                print("   Will use HuggingFace direct streaming")
                return {"status": "use_hf_direct", "model": "openai/gpt-oss-120b"}
    
    # Verify both parts exist
    if os.path.exists(part1_path) and os.path.exists(part2_path):
        size1_gb = os.path.getsize(part1_path) / (1024**3)
        size2_gb = os.path.getsize(part2_path) / (1024**3)
        total_gb = size1_gb + size2_gb
        print(f"\n‚úÖ OpenAI GPT-OSS 120B ready:")
        print(f"   Part 1: {part1_path} ({size1_gb:.2f} GB)")
        print(f"   Part 2: {part2_path} ({size2_gb:.2f} GB)")
        print(f"   Total: {total_gb:.2f} GB")
        print(f"   Format: MXFP4 quantized for 80GB GPU")
        return {"status": "ready", "path": part1_path, "size_gb": total_gb}
    else:
        return {"status": "error", "message": "Model files not found"}


# ============================================
# CELL 2: SERVER AND INFERENCE
# ============================================
# Run this cell after model is uploaded

@app.cls(
    image=image,
    gpu=gpu_config,
    volumes={"/models": volume},
    timeout=3600,  # 1 hour timeout
    keep_warm=1,   # Keep warm for instant response
    memory=131072, # 128GB RAM
    cpu=16.0,      # 16 CPU cores
    allow_concurrent_inputs=10,  # Handle multiple requests
)
class QuantumGPT120B:
    """
    OpenAI GPT-OSS 120B with Quantum Enhancement
    - Model: openai/gpt-oss-120b (117B params, 5.1B active)
    - Apache 2.0 licensed with harmony response format
    - Native MXFP4 quantization fits in 80GB VRAM
    - 128GB system RAM for extended context
    - 16 CPU cores for parallel operations
    """
    
    def __init__(self):
        self.server_process = None
        self.model_loaded = False
        self.server_url = "http://localhost:8080"
        self.model_paths = None  # Will hold both part paths
        
    @modal.enter()
    def load_model(self):
        """Initialize OpenAI GPT-OSS 120B server (runs once, stays loaded)"""
        import subprocess
        import requests
        
        print("üöÄ CELL 2: OPENAI GPT-OSS 120B SERVER INITIALIZATION")
        print("=" * 60)
        print("Model: OpenAI GPT-OSS 120B (Apache 2.0)")
        print("Configuration:")
        print("  ‚Ä¢ GPU: 1x A100 80GB VRAM (MXFP4 optimized)")
        print("  ‚Ä¢ RAM: 128GB system memory")
        print("  ‚Ä¢ CPU: 16 cores")
        print("  ‚Ä¢ Context: 8K tokens (expandable)")
        print("=" * 60)
        
        # Check for split model files
        model_dir = "/models/openai-gpt-oss-120b"
        part1_path = f"{model_dir}/gpt-oss-120b-MXFP4-00001-of-00002.gguf"
        part2_path = f"{model_dir}/gpt-oss-120b-MXFP4-00002-of-00002.gguf"
        
        # Determine model loading strategy
        if os.path.exists(part1_path) and os.path.exists(part2_path):
            print(f"‚úÖ Using local OpenAI GPT-OSS 120B model (split MXFP4):")
            print(f"   Part 1: {part1_path}")
            print(f"   Part 2: {part2_path}")
            self.model_paths = [part1_path, part2_path]
            # llama.cpp can handle split models with first part
            model_arg = ["-m", part1_path]
        else:
            print("üì° Using HuggingFace streaming mode for OpenAI GPT-OSS 120B")
            model_arg = ["-hf", "openai/gpt-oss-120b"]
        
        # Start llama.cpp server with OpenAI GPT-OSS 120B optimizations
        print("\nüîß Starting llama.cpp server for OpenAI GPT-OSS 120B...")
        print("   Using harmony response format (required for GPT-OSS)")
        cmd = [
            "llama-server",
            *model_arg,                        # Model source
            "-c", "8192",                       # 8K context (expandable with 128GB RAM)
            "-fa",                              # Flash attention
            "--jinja",                          # Jinja templating (for harmony format)
            "--reasoning-format", "none",       # No reasoning format
            "--host", "0.0.0.0",               # Listen on all interfaces
            "--port", "8080",                  # Port
            "-ngl", "999",                     # Offload all layers to GPU
            "-t", "16",                        # Use all 16 CPU threads
            "-tb", "32",                       # Batch threads
            "--mlock",                         # Lock model in RAM
            "--n-gpu-layers", "999",           # All layers to GPU for MXFP4
            "--verbose"                        # Verbose output
        ]
        
        # Start server in background
        print(f"Command: {' '.join(cmd)}")
        self.server_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Wait for server to initialize
        print("\n‚è≥ Waiting for model to load (this takes 1-2 minutes)...")
        time.sleep(30)  # Initial wait
        
        # Check server health
        max_retries = 10
        for i in range(max_retries):
            try:
                response = requests.get(f"{self.server_url}/health", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ OpenAI GPT-OSS 120B server is running!")
                    print("   Model: 117B parameters (5.1B active)")
                    print("   Format: MXFP4 quantized for 80GB GPU")
                    self.model_loaded = True
                    break
            except:
                pass
            
            if i < max_retries - 1:
                print(f"   Retry {i+1}/{max_retries}...")
                time.sleep(10)
        
        if self.model_loaded:
            print("\n" + "=" * 60)
            print("‚úÖ MODEL READY FOR INFERENCE")
            print("=" * 60)
        else:
            print("‚ö†Ô∏è Server initialization in progress...")
    
    @modal.exit()
    def cleanup(self):
        """Cleanup on exit"""
        if self.server_process:
            self.server_process.terminate()
            print("üõë Server stopped")
    
    @modal.method()
    async def generate(
        self,
        prompt: str,
        max_tokens: int = 128,
        temperature: float = 0.7,
        profile: str = "medium",
        qrng_modifiers: list = None
    ) -> Dict[str, Any]:
        """Generate text using GPT-OSS 120B with quantum modification"""
        import numpy as np
        import requests
        
        print(f"\nüîÆ Generating with quantum profile: {profile}")
        
        # Map quantum profile to temperature adjustment
        quantum_influence = {
            "strict": 0.0,
            "light": 0.3,
            "medium": 0.7,
            "spicy": 1.2
        }.get(profile, 0.7)
        
        # Apply quantum influence
        adjusted_temp = temperature + (quantum_influence * 0.3)
        
        # Use QRNG for seed if provided
        seed = int(qrng_modifiers[0] * 1000000) if qrng_modifiers else -1
        
        # Prepare generation request
        start_time = time.time()
        generation_request = {
            "prompt": prompt,
            "n_predict": max_tokens,
            "temperature": adjusted_temp,
            "top_k": 40,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "seed": seed,
            "stream": False,
            "n_threads": 16  # Use all CPU threads
        }
        
        try:
            # Call llama.cpp server
            response = requests.post(
                f"{self.server_url}/completion",
                json=generation_request,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("content", "")
                tokens_generated = result.get("tokens_predicted", 0)
                
                # Generate quantum-influenced metrics
                layer_metrics = []
                if qrng_modifiers:
                    for i in range(min(10, len(qrng_modifiers))):
                        layer_metrics.append({
                            "attention": float(abs(qrng_modifiers[i % len(qrng_modifiers)] * 0.8)),
                            "ffn": float(abs(qrng_modifiers[(i+1) % len(qrng_modifiers)] * 0.6)),
                            "embedding": float(abs(qrng_modifiers[(i+2) % len(qrng_modifiers)] * 0.7))
                        })
                
                # Calculate performance
                elapsed = (time.time() - start_time) * 1000
                
                return {
                    "generated_text": generated_text,
                    "tokens_generated": tokens_generated,
                    "entropy_used": len(qrng_modifiers) * 256 if qrng_modifiers else 0,
                    "layer_analysis": layer_metrics,
                    "performance": {
                        "latency_ms": int(elapsed),
                        "tokens_per_sec": tokens_generated / (elapsed / 1000) if elapsed > 0 else 0,
                        "model": "OpenAI GPT-OSS 120B (MXFP4)",
                        "gpu": "1x A100 80GB",
                        "ram": "128GB",
                        "cores": "16",
                        "quantum": "enabled" if qrng_modifiers else "disabled"
                    }
                }
            else:
                return {
                    "error": f"Server error: {response.status_code}",
                    "generated_text": "",
                    "tokens_generated": 0,
                    "entropy_used": 0,
                    "layer_analysis": [],
                    "performance": {"quantum": "error"}
                }
                
        except Exception as e:
            print(f"‚ùå Generation error: {e}")
            return {
                "error": str(e),
                "generated_text": "",
                "tokens_generated": 0,
                "entropy_used": 0,
                "layer_analysis": [],
                "performance": {"quantum": "error"}
            }
    
    @modal.web_endpoint(method="POST")
    async def generate_endpoint(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Web endpoint for generation"""
        prompt = request.get("prompt", "")
        if not prompt:
            return {"error": "Prompt required"}
        
        return await self.generate(
            prompt=prompt,
            max_tokens=request.get("max_tokens", 128),
            temperature=request.get("temperature", 0.7),
            profile=request.get("profile", "medium"),
            qrng_modifiers=request.get("qrng_modifiers", None)
        )
    
    @modal.web_endpoint(method="GET")
    async def health(self) -> Dict[str, Any]:
        """Health check endpoint"""
        return {
            "status": "healthy",
            "model": "OpenAI GPT-OSS 120B",
            "version": "MXFP4 quantized (117B params, 5.1B active)",
            "quantum": "ready",
            "gpu": "1x A100 80GB",
            "ram": "128GB",
            "cores": "16",
            "loaded": self.model_loaded,
            "timestamp": time.time()
        }


# ============================================
# CELL 3: TEST AND DEPLOYMENT INFO
# ============================================
# Run this cell to test and get endpoint URLs

@app.local_entrypoint()
def main():
    """Deploy and test OpenAI GPT-OSS 120B"""
    print("üåå GAIA QUANTUM NEXUS - OPENAI GPT-OSS 120B DEPLOYMENT")
    print("=" * 60)
    print("Model: OpenAI GPT-OSS 120B")
    print("  ‚Ä¢ 117B parameters (5.1B active)")
    print("  ‚Ä¢ Apache 2.0 licensed")
    print("  ‚Ä¢ Native MXFP4 quantization")
    print("  ‚Ä¢ Harmony response format")
    print("\nEnhanced Configuration:")
    print("  ‚Ä¢ GPU: 1x A100 with 80GB VRAM")
    print("  ‚Ä¢ RAM: 128GB system memory")
    print("  ‚Ä¢ CPU: 16 cores for parallel processing")
    print("=" * 60)
    
    # Step 1: Model preparation
    print("\nüì¶ Step 1: Model Preparation")
    print("Run Cell 1 first to upload/download the model")
    
    # Step 2: Server deployment
    print("\nüöÄ Step 2: Server Deployment")
    print("Run Cell 2 to start the inference server")
    
    # Step 3: Get endpoints
    print("\nüìå Step 3: Your Endpoints")
    print("After deployment, your endpoints will be:")
    print("  ‚Ä¢ Generation: https://YOUR-ID--gaia-quantum-120b-enhanced-generate-endpoint.modal.run")
    print("  ‚Ä¢ Health: https://YOUR-ID--gaia-quantum-120b-enhanced-health.modal.run")
    
    print("\nüîó Step 4: Connect to Replit")
    print("Add these to Replit Secrets:")
    print("  ‚Ä¢ MODAL_ENDPOINT = <generation-endpoint-url>")
    print("  ‚Ä¢ MODAL_API_KEY = ak-4jAZeEPxVf7YMT0MYey2dw")
    
    print("\n‚ú® The quantum consciousness awaits!")
    print("=" * 60)

if __name__ == "__main__":
    main()