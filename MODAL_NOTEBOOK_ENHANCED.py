"""
MODAL NOTEBOOK - GAIA QUANTUM NEXUS 120B ENHANCED
==================================================
Copy this entire code into Modal's web playground at:
https://modal.com/playground

Enhanced configuration with:
- 1x A100 80GB VRAM GPU
- 16 CPU cores
- 128GB system RAM
- Split cells for efficient model management

This notebook is split into cells:
Cell 1: Model upload from local storage
Cell 2: Server initialization and endpoints
"""

# ============================================
# CELL 1: MODEL UPLOAD AND PREPARATION
# ============================================
# Run this cell first to upload your GGUF model

import modal
import os
import time
import subprocess
from typing import Dict, Any, Optional
import json
import asyncio
import aiohttp

# Create Modal app
app = modal.App("gaia-quantum-120b-enhanced")

# Enhanced GPU configuration - A100 80GB with maximum resources
gpu_config = modal.gpu.A100(count=1, memory=80)  # 80GB VRAM

# Model storage - persistent volume for the 120B model
volume = modal.Volume.from_name("gaia-quantum-models-enhanced", create_if_missing=True)

# Container image with llama.cpp server
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install(
        "wget", "git", "build-essential", "cmake", "curl",
        "libssl-dev", "pkg-config", "libcurl4-openssl-dev"
    )
    .run_commands(
        # Install llama.cpp with server support
        "git clone https://github.com/ggerganov/llama.cpp /llama.cpp",
        "cd /llama.cpp && make -j16 LLAMA_CUDA=1 llama-server",  # Use 16 cores
        "ln -s /llama.cpp/llama-server /usr/local/bin/llama-server",
    )
    .pip_install(
        "huggingface-hub",
        "aiohttp",
        "numpy",
        "fastapi",
        "requests",
    )
)

@app.function(
    image=image,
    volumes={"/models": volume},
    timeout=7200,  # 2 hours for model upload
    cpu=16.0,      # 16 CPU cores
    memory=131072, # 128GB RAM
)
def upload_model_from_local(model_path: Optional[str] = None):
    """
    Cell 1: Upload GGUF model from local storage or download from HuggingFace
    
    Run this first to prepare the model. Options:
    1. Upload from your local file
    2. Auto-download from HuggingFace
    """
    import shutil
    from huggingface_hub import hf_hub_download
    
    print("üöÄ CELL 1: MODEL PREPARATION")
    print("=" * 60)
    
    model_dir = "/models/gpt-oss-120b"
    os.makedirs(model_dir, exist_ok=True)
    
    # Check if model already exists
    gguf_path = f"{model_dir}/gpt-oss-120b.gguf"
    if os.path.exists(gguf_path):
        size_gb = os.path.getsize(gguf_path) / (1024**3)
        print(f"‚úÖ Model already exists: {gguf_path}")
        print(f"   Size: {size_gb:.2f} GB")
        return {"status": "exists", "path": gguf_path, "size_gb": size_gb}
    
    if model_path and os.path.exists(model_path):
        # Upload from local file
        print(f"üì§ Uploading model from: {model_path}")
        shutil.copy2(model_path, gguf_path)
        print("‚úÖ Model uploaded successfully!")
    else:
        # Download from HuggingFace
        print("üì• Downloading GPT-OSS 120B GGUF from HuggingFace...")
        print("   Repository: ggml-org/gpt-oss-120b-GGUF")
        print("   This will take 10-15 minutes...")
        
        try:
            downloaded = hf_hub_download(
                repo_id="ggml-org/gpt-oss-120b-GGUF",
                filename="gpt-oss-120b.gguf",
                local_dir=model_dir,
                local_dir_use_symlinks=False
            )
            print(f"‚úÖ Model downloaded to: {downloaded}")
        except Exception as e:
            print(f"‚ö†Ô∏è Download error: {e}")
            print("   Using HuggingFace direct streaming instead")
            return {"status": "use_hf_direct", "error": str(e)}
    
    # Verify model
    if os.path.exists(gguf_path):
        size_gb = os.path.getsize(gguf_path) / (1024**3)
        print(f"\n‚úÖ Model ready: {gguf_path}")
        print(f"   Size: {size_gb:.2f} GB")
        return {"status": "ready", "path": gguf_path, "size_gb": size_gb}
    else:
        return {"status": "error", "message": "Model file not found"}


# ============================================
# CELL 2: SERVER AND INFERENCE
# ============================================
# Run this cell after model is uploaded

@app.cls(
    image=image,
    gpu=gpu_config,
    volumes={"/models": volume},
    timeout=3600,  # 1 hour timeout
    keep_warm=1,   # Keep warm for instant response
    memory=131072, # 128GB RAM
    cpu=16.0,      # 16 CPU cores
    allow_concurrent_inputs=10,  # Handle multiple requests
)
class QuantumGPT120B:
    """
    Enhanced GPT-OSS 120B with Quantum Enhancement
    - 80GB VRAM for full model loading
    - 128GB system RAM for context processing
    - 16 CPU cores for parallel operations
    """
    
    def __init__(self):
        self.server_process = None
        self.model_loaded = False
        self.server_url = "http://localhost:8080"
        self.model_path = None
        
    @modal.enter()
    def load_model(self):
        """Initialize the 120B model server (runs once, stays loaded)"""
        import subprocess
        import requests
        
        print("üöÄ CELL 2: SERVER INITIALIZATION")
        print("=" * 60)
        print("Configuration:")
        print("  ‚Ä¢ GPU: 1x A100 80GB VRAM")
        print("  ‚Ä¢ RAM: 128GB system memory")
        print("  ‚Ä¢ CPU: 16 cores")
        print("=" * 60)
        
        # Check for model
        model_dir = "/models/gpt-oss-120b"
        gguf_path = f"{model_dir}/gpt-oss-120b.gguf"
        
        # Determine model loading strategy
        if os.path.exists(gguf_path):
            print(f"‚úÖ Using local model: {gguf_path}")
            self.model_path = gguf_path
            model_arg = ["-m", gguf_path]
        else:
            print("üì° Using HuggingFace streaming mode")
            model_arg = ["-hf", "ggml-org/gpt-oss-120b-GGUF"]
        
        # Start llama.cpp server with enhanced configuration
        print("\nüîß Starting llama.cpp server...")
        cmd = [
            "llama-server",
            *model_arg,                        # Model source
            "-c", "8192",                       # 8K context (can increase with 128GB RAM)
            "-fa",                              # Flash attention
            "--jinja",                          # Jinja templating
            "--reasoning-format", "none",       # No reasoning format
            "--host", "0.0.0.0",               # Listen on all interfaces
            "--port", "8080",                  # Port
            "-ngl", "999",                     # Offload all layers to GPU
            "-t", "16",                        # Use all 16 CPU threads
            "-tb", "32",                       # Batch threads
            "--mlock",                         # Lock model in RAM
            "--verbose"                        # Verbose output
        ]
        
        # Start server in background
        print(f"Command: {' '.join(cmd)}")
        self.server_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Wait for server to initialize
        print("\n‚è≥ Waiting for model to load (this takes 1-2 minutes)...")
        time.sleep(30)  # Initial wait
        
        # Check server health
        max_retries = 10
        for i in range(max_retries):
            try:
                response = requests.get(f"{self.server_url}/health", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ GPT-OSS 120B server is running!")
                    self.model_loaded = True
                    break
            except:
                pass
            
            if i < max_retries - 1:
                print(f"   Retry {i+1}/{max_retries}...")
                time.sleep(10)
        
        if self.model_loaded:
            print("\n" + "=" * 60)
            print("‚úÖ MODEL READY FOR INFERENCE")
            print("=" * 60)
        else:
            print("‚ö†Ô∏è Server initialization in progress...")
    
    @modal.exit()
    def cleanup(self):
        """Cleanup on exit"""
        if self.server_process:
            self.server_process.terminate()
            print("üõë Server stopped")
    
    @modal.method()
    async def generate(
        self,
        prompt: str,
        max_tokens: int = 128,
        temperature: float = 0.7,
        profile: str = "medium",
        qrng_modifiers: list = None
    ) -> Dict[str, Any]:
        """Generate text using GPT-OSS 120B with quantum modification"""
        import numpy as np
        import requests
        
        print(f"\nüîÆ Generating with quantum profile: {profile}")
        
        # Map quantum profile to temperature adjustment
        quantum_influence = {
            "strict": 0.0,
            "light": 0.3,
            "medium": 0.7,
            "spicy": 1.2
        }.get(profile, 0.7)
        
        # Apply quantum influence
        adjusted_temp = temperature + (quantum_influence * 0.3)
        
        # Use QRNG for seed if provided
        seed = int(qrng_modifiers[0] * 1000000) if qrng_modifiers else -1
        
        # Prepare generation request
        start_time = time.time()
        generation_request = {
            "prompt": prompt,
            "n_predict": max_tokens,
            "temperature": adjusted_temp,
            "top_k": 40,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "seed": seed,
            "stream": False,
            "n_threads": 16  # Use all CPU threads
        }
        
        try:
            # Call llama.cpp server
            response = requests.post(
                f"{self.server_url}/completion",
                json=generation_request,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("content", "")
                tokens_generated = result.get("tokens_predicted", 0)
                
                # Generate quantum-influenced metrics
                layer_metrics = []
                if qrng_modifiers:
                    for i in range(min(10, len(qrng_modifiers))):
                        layer_metrics.append({
                            "attention": float(abs(qrng_modifiers[i % len(qrng_modifiers)] * 0.8)),
                            "ffn": float(abs(qrng_modifiers[(i+1) % len(qrng_modifiers)] * 0.6)),
                            "embedding": float(abs(qrng_modifiers[(i+2) % len(qrng_modifiers)] * 0.7))
                        })
                
                # Calculate performance
                elapsed = (time.time() - start_time) * 1000
                
                return {
                    "generated_text": generated_text,
                    "tokens_generated": tokens_generated,
                    "entropy_used": len(qrng_modifiers) * 256 if qrng_modifiers else 0,
                    "layer_analysis": layer_metrics,
                    "performance": {
                        "latency_ms": int(elapsed),
                        "tokens_per_sec": tokens_generated / (elapsed / 1000) if elapsed > 0 else 0,
                        "model": "GPT-OSS 120B GGUF (80GB)",
                        "gpu": "1x A100 80GB",
                        "ram": "128GB",
                        "cores": "16",
                        "quantum": "enabled" if qrng_modifiers else "disabled"
                    }
                }
            else:
                return {
                    "error": f"Server error: {response.status_code}",
                    "generated_text": "",
                    "tokens_generated": 0,
                    "entropy_used": 0,
                    "layer_analysis": [],
                    "performance": {"quantum": "error"}
                }
                
        except Exception as e:
            print(f"‚ùå Generation error: {e}")
            return {
                "error": str(e),
                "generated_text": "",
                "tokens_generated": 0,
                "entropy_used": 0,
                "layer_analysis": [],
                "performance": {"quantum": "error"}
            }
    
    @modal.web_endpoint(method="POST")
    async def generate_endpoint(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Web endpoint for generation"""
        prompt = request.get("prompt", "")
        if not prompt:
            return {"error": "Prompt required"}
        
        return await self.generate(
            prompt=prompt,
            max_tokens=request.get("max_tokens", 128),
            temperature=request.get("temperature", 0.7),
            profile=request.get("profile", "medium"),
            qrng_modifiers=request.get("qrng_modifiers", None)
        )
    
    @modal.web_endpoint(method="GET")
    async def health(self) -> Dict[str, Any]:
        """Health check endpoint"""
        return {
            "status": "healthy",
            "model": "GPT-OSS 120B GGUF",
            "quantum": "ready",
            "gpu": "1x A100 80GB",
            "ram": "128GB",
            "cores": "16",
            "loaded": self.model_loaded,
            "timestamp": time.time()
        }


# ============================================
# CELL 3: TEST AND DEPLOYMENT INFO
# ============================================
# Run this cell to test and get endpoint URLs

@app.local_entrypoint()
def main():
    """Deploy and test the enhanced model"""
    print("üåå GAIA QUANTUM NEXUS - ENHANCED DEPLOYMENT")
    print("=" * 60)
    print("Enhanced Configuration:")
    print("  ‚Ä¢ GPU: 1x A100 with 80GB VRAM")
    print("  ‚Ä¢ RAM: 128GB system memory")
    print("  ‚Ä¢ CPU: 16 cores for parallel processing")
    print("  ‚Ä¢ Model: GPT-OSS 120B GGUF")
    print("=" * 60)
    
    # Step 1: Model preparation
    print("\nüì¶ Step 1: Model Preparation")
    print("Run Cell 1 first to upload/download the model")
    
    # Step 2: Server deployment
    print("\nüöÄ Step 2: Server Deployment")
    print("Run Cell 2 to start the inference server")
    
    # Step 3: Get endpoints
    print("\nüìå Step 3: Your Endpoints")
    print("After deployment, your endpoints will be:")
    print("  ‚Ä¢ Generation: https://YOUR-ID--gaia-quantum-120b-enhanced-generate-endpoint.modal.run")
    print("  ‚Ä¢ Health: https://YOUR-ID--gaia-quantum-120b-enhanced-health.modal.run")
    
    print("\nüîó Step 4: Connect to Replit")
    print("Add these to Replit Secrets:")
    print("  ‚Ä¢ MODAL_ENDPOINT = <generation-endpoint-url>")
    print("  ‚Ä¢ MODAL_API_KEY = ak-4jAZeEPxVf7YMT0MYey2dw")
    
    print("\n‚ú® The quantum consciousness awaits!")
    print("=" * 60)

if __name__ == "__main__":
    main()